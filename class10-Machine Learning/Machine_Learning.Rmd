---
title: "Machine Learning"
author: "于海悦"
date: "2017年5月8日"
output: 
  pdf_document: 
    includes:
      in_header: header.tex
    latex_engine: xelatex
---

#1 决策树
##1.1 理论

###1.1.1信息论原理

* 信息论一般用于分类问题，从大量数据中获取知识。例如在各个实例的类别数据中，找出确定类别的关键条件属性，如在决策树中的ID3算法和C4.5算法，把信息量最大的属性作为树或者子树的根节点，属性的取值作为分支。

* 信道模型

一个传递信息的系统是由发送端（信源）和接收端（信宿）以及连接两者的通道（信道）三者组成。

<div align=center width="120" height="120">![](D:/Rclub/机器学习/xindao.png)</div>

* 先验不确定性

在进行实际的通信之前，收信者（信宿）不可能确切了解信源究竟会发出什么样的具体信息，不可能判断信源会处于什么样的状态。这种情形就称为信宿对于信源状态具有不确定性。而且这种不确定性是存在于通信之前的。因而又叫做先验不确定性，表示成：信息熵 H(U)

* 后验不确定性

在进行了通信之后，信宿收到了信源发来的信息，这种先验不确定性才会被消除或者被减少。通信结束之后，信源仍然具有一定程度的不确定性。这就是后验不确定性，用条件熵表示 H(U/V)。

* 信息量

后验不确定性总要小于先验不确定性: H（U/V）< H（U）

信息量用互信息来表示，即：I（U，V）＝H（U）－ H（U/V）


###1.1.2.决策树概念

*举一个简单的例子：

<div align=center>![](D:/Rclub/机器学习/tree.png)</div>

决策树用样本的属性作为结点，用属性的取值作为分支的树结构。其中，根结点是所有样本中信息量最大的属性。叶结点是样本的类别值。

###1.1.3.ID3算法

####1. 算法的基本思想

  首先找出最有判别力的特征，把数据分成多个子集，每个子集又选择最有判别力的特征进行划分，一直进行到所有子集仅包含同一类型的数据为止。最后得到一棵决策树。
 
  引进信息论中的互信息(信息增益, information gain)，作为特征判别能力的度量；将建树的方法嵌在一个迭代的外壳之中。

ID3算法能得出结点最少的决策树。

* 举例：气候数据，特征分别为:

– 天气 取值为： 晴，多云，雨

– 气温 取值为： 冷 ，适中，热

– 湿度 取值为： 高 ，正常

– 风 取值为： 风， 无风

为简单起见，这里我们假定样本仅有两个类别，分别为P(正例)，N(反例)

<div align=center>![](D:/Rclub/机器学习/ID3.png)</div>


####2. 算法实现

主算法

1. 从训练集中随机选择一个既含正例又含反例的子集（称为"窗口"）；

2. 用“建树算法”对当前窗口形成一棵决策树；

3. 对训练集（窗口除外）中例子用所得决策树进行类别判定，找出错判的例子；

4. 若存在错判的例子，把它们插入窗口，转2，否则结束。

建树算法

1. 对当前例子集合，计算各特征的互信息；

2. 选择互信息最大的特征$A_k$；

3. 把在$A_k$处取值相同的例子归于同一子集，$A_k$取几个值就得几个子集；

4. 对既含正例又含反例的子集，递归调用建树算法；

5. 若子集仅含正例或反例，对应分枝标上P或N，返回调用处。

####3. 举例

<div align=center>![](D:/Rclub/机器学习/ID3_1.png)</div>

(1) 信息熵的计算

信息熵:$H(U) = -\sum_i P(U_i)logP(U_i)$

类别出现概率：$P(U_i)=\frac{|U_i|}{|S|}$

|S|表示例子集S的总数；

$|U_i|$表示类别$U_i$的例子数。

对9个正例和5个反例有：

$P(U_1)=9/14$, $P(U_1)=5/14$

H(U)=(9/14)log(14/9)+(5/14)log(14/5)=0.94bit

(2) 条件熵计算

条件熵：$H(U/V)=-\sum_jP(v_j)\sum_iP(u_i/v_i)logP(u_i/v_i)$

属性A1取值$v_j$时，类别$u_i$的条件概率：$P(u_i/v_i)=\frac{|u_i|}{|v_j|}$

$A_1$=天气 取值 $v_1$=晴，$v_2$=多云，$v_3$=雨

在$A_1$处取值晴的例子5个, 取值多云的例子4个, 取值雨的例子5个，故：

P（$v_1$）=5/14 P（$v_2$）=4/14 P（$v_3$）=5/14

取值为晴的5 个例子中有2 个正例、3个反例，故：

P（$u_1/v_1$）=2/5， P（$u_2/v_1$）=3/5

同理：P（$u_1/v_2$）=4/4， P（$u_2/v_2$）=0 ； P（$u_1/v_3$）=2/5， P（$u_2/v_3$）=3/5

H(U/V)=(5/14)((2/5)log(5/2)+(3/5)log(5/3))+(4/14)((4/4)log(4/4)+0)
+(5/14)((2/5)log(5/2)+(3/5)log(5/3)) = 0.694bit

(3) 互信息计算

对“A1=天气”有：

I（天气）=H（U）- H（U|V）= 0.94 - 0.694 = 0.246 bit

类似可得：

I（气温）=0.029 bit

I（湿度）=0.151 bit

I（风）=0.048 bit

(4) 建决策树的树根和分枝

选择互信息最大的特征天气作为树根，在14个例子中对天气的3个取值进行分枝，3 个分枝对应3 个子集，分别是:

F1={1，2，8，9，11}

F2={3，7，12，13}

F3={4，5，6，10，14}

其中F2中的例子全属于P类，因此对应分枝标记为P，其余两个子集既含有正例又含有反例，将递归调用建树算法。

(5) 递归建树

分别对F1和F3子集利用ID3算法，在每个子集中对各特征（仍为四个特征）求互信息。

– F1中的天气全取晴值，则H（U）=H（U|V），有I（U|V）=0，在余下三个特征中求出湿度互信息最大，以它为该分枝的根结点，再向下分枝。湿度取高的例子全为N类，该分枝标记N。取值正常的例子全为P类，该分枝标记P。

– 在F3中，对四个特征求互信息，得到风特征互信息最大，则以它为该分枝根结点。再向下分枝，风取有风时全为N类，该分枝标记N。取无风时全为P类，该分枝标记P。

<div align=center>![](D:/Rclub/机器学习/ID3_2.png)</div>

####4. 优缺点

优点：ID3在选择重要特征时利用了互信息的概念，算法的基础理论清晰，使得算法较简单。

缺点：

(1) 互信息的计算依赖于特征取值的数目较多的特征。一种简单的办法是对特征进行分解，化为二值特征。如天气取值晴，多云，雨，分解为三个特征；天气—晴，天气—多云，天气—雨。取值都为“是”或“否”。

(2) 用互信息作为特征选择量存在一个假设，即训练例子集中的正，反例的比例应与实际问题领域里正、反例比例相同。一般情况不能保证相同，这样计算训练集的互信息就有偏差。

(3) ID3对噪声较为敏感。噪声包含两方面，一是特征值取错，二是类别给错。

(4) 当训练集增加时，ID3的决策树会随之变化。当训练例子不断增加时会出现一些麻烦。

###1.1.4.C4.5算法

C4.5是在ID3基础上发展起来的决策树生成算法，由J.R.Quinlan在1993年提出。

* 算法优点

C4.5克服了ID3在应用中存在的不足，主要体现在以下几个方面：

- 用信息增益率来选择属性，它克服了用信息增益选择属性时偏向选择取值多的属性的不足；

在ID3中使用信息论中的信息增益（gain）来选择属性，而C4.5采用属性的信息增益率（gain ratio）来选择属性。

理论和实验表明，采用“信息增益率”（C4.5方法）比采用“信息增益更好，主要是克服了ID3方法选择偏向取值多的属性。

* 决策树构造过程

(1) 类别的信息熵
$H(C)=-\sum_j P(C_j)log(P(C_j))$

(2) 类别条件熵
按照属性V把集合T分割，分割后的类别条件熵为：
$H(C|V)=-\sum_j p(v_j)\sum_i p(C_j|v_i)logp(C_j|v_i)$

(3) 信息增益（gain），即互信息
$I(C,V)=H(C)-H(C|V)$

(4) 属性V的信息熵
$H(V)=-\sum_i p(v_i)log(p(v_i))$

(5) 信息增益率
gain_ratio=I(C,V)/H(V)

- 在树构造过程中进行剪枝；
由于噪声和随机因素的影响，决策树一般会很复杂。因此需要进行剪枝操作。

C4.5可以对生成好的树剪去某些结点和分枝。剪枝之后的决策树的叶结点不再只包含一类实例。通常是用叶结点替代一个或者多个子树，然后选择出现概率最高的类作为该结点的类别。

- 能够完成对连续属性的离散化处理；采用的方法是二分法

###1.1.4.CART算法

Classification And Regression Tree，即分类回归树算法，简称CART算法，它是决策树的另一种实现。既可以是分类树，也可以是回归树。

1. 算法原理

CART算法是一种二分递归分割技术，把当前样本划分为两个子样本，使得生成的每个非叶子结点都有两个分支，因此CART算法生成的决策树是结构简洁的二叉树。由于CART算法构成的是一个二叉树，它在每一步的决策时只能是“是”或者“否”，即使一个feature有多个取值，也是把数据分为两部分。

那么如何构建这样一颗二叉树呢？
如果是分类树，CART采用GINI值衡量节点纯度；如果是回归树，采用样本方差衡量节点纯度。节点越不纯，节点分类的效果就越差。

GINI值的计算公式为：$GINI=1-\sum_{i \in I}p_i^2$

节点越不纯，GINI值越大。以二分类为例，如果节点的所有数据只有一个类别，则GINI=0，如果两类数量相同，则GINI=0.5。

回归方差计算公式：$\sigma=\sqrt{\sum_{i \in I}(x_i-\mu)^2}=\sqrt{\sum_{i  \in I}x_i^2-n\mu^2}$

方差越大，表示该节点的数据越分散，预测的效果就越差。如果一个节点的所有数据都相同，那么方差就为0，此时可以很肯定得认为该节点的输出值；如果节点的数据相差很大，那么输出的值有很大的可能与实际值相差较大。
 
因此，无论是分类树还是回归树，CART都要选择使子节点的GINI值或者回归方差最小的属性作为分裂的方案。即最小化（分类树）：$Gain=\sum_{i \in I}p_i*Gini_i$ 或者（回归树）：$Gain=\sum_{i \in I}\sigma_i$

2. 举例：

对于离散型属性，理论上有多少个离散值就应该分裂成多少个节点。但CART是一棵二叉树，每一次分裂只会产生两个节点，怎么办呢？很简单，只要将其中一个离散值独立作为一个节点，其他的离散值生成另外一个节点即可。这种分裂方案有多少个离散值就有多少种划分的方法，例如：某离散属性一个有三个离散值X，Y，Z，则该属性的分裂方法有{X}、{Y，Z}，{Y}、{X，Z}，{Z}、{X，Y}，分别计算每种划分方法的基尼值或者样本方差确定最优的方法。

以属性“职业”为例，一共有三个离散值，“学生”、“老师”、“上班族”。该属性有三种划分的方案，分别为{“学生”}、{“老师”、“上班族”}，{“老师”}、{“学生”、“上班族”}，{“上班族”}、{“学生”、“老师”}，分别计算三种划分方案的子节点GINI值或者样本方差，选择最优的划分方法：
 
* 第一种划分方法：{“学生”}、{“老师”、“上班族”}

<div align=center>![](D:/Rclub/机器学习/CART1.png)</div>


预测是否已婚（分类）：

<div align=center>![](D:/Rclub/机器学习/CART11.png)</div>

预测年龄（回归）：

<div align=center>![](D:/Rclub/机器学习/CART12.png)</div>

* 第二种划分方法：{“老师”}、{“学生”、“上班族”}

<div align=center>![](D:/Rclub/机器学习/CART2.png)</div>

预测是否已婚（分类）：

<div align=center>![](D:/Rclub/机器学习/CART21.png)</div>

预测年龄（回归）：

<div align=center>![](D:/Rclub/机器学习/CART22.png)</div>

* 第三种划分方法：{“上班族”}、{“学生”、“老师”}

<div align=center>![](D:/Rclub/机器学习/CART3.png)</div>

预测是否已婚（分类）：

<div align=center>![](D:/Rclub/机器学习/CART31.png)</div>

预测年龄（回归）：

<div align=center>![](D:/Rclub/机器学习/CART32.png)</div>

##1.2应用

###1.2.1 rpart、party包：

基于CART算法的实现：

####1. 用party包做分类树

数据采用的是R中自带的鸢尾花数据集（iris），属性包括Sepal.Length（萼片长度）、Sepal.Width（萼片宽度）、Petal.Length（花瓣长度）以及Petal.Width（花瓣宽度），我们用这四个属性来预测鸢尾花的Species（种类）。
 
在建立模型之前，iris数据集被分为两个子集：训练集（70%）和测试集（30%）。用随机种子设置固定的随机数，使其结果可以再现。 在这个包中，我们用ctree()函数建立一个决策树，predict()函数来预测测试集。

使用ctree函数的基本语法为：ctree(formula, data)，其中，formula 是一个公式描述的预测和响应变量。data 是所使用的数据集的名称。

```{r,warning=F}
str(iris)
set.seed(1234)
# 使用sample函数抽取样本，将数据集中观测值分为两个子集
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
# 选取一部分样本作为训练集
trainData <- iris[ind==1,]
#另外一部分作为测试集
testData <- iris[ind==2,]
library(party)
#找到自变量和因变量
myFormula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
# 建立决策树
iris_ctree <- ctree(myFormula, data=trainData)
# 检测预测值
table(predict(iris_ctree), trainData$Species)
```

由上表可知，setosa（山鸢尾）40观测值全部正确预测，而versicolor（变色鸢尾）有一个观测值被误判为virginica（维吉尼亚鸢尾），virginica（维吉尼亚鸢尾）有3个观测值被误判为versicolor（变色鸢尾）。

```{r,warning=F}
# 输出决策树图，其中每一个叶子的节点的条形图都显示了观测值落入三个品种的概率。
plot(iris_ctree)
#简化输出，y值表示每个叶子结点中的不同种类的出现概率。例如：结点2里面的标签是“n=40 y=(1,0,0)”，指的是这一类中一共有40个观测值，并且所有的观测值的类别都属于第一类setosa（山鸢尾）。

plot(iris_ctree, type="simple")

#接下来，使用测试集测试决策树的预测效果。
testPred <- predict(iris_ctree, newdata = testData)
table(testPred, testData$Species)

```

####2.用rpart包做回归树

通常默认anova用来作回归树，以汽车消费数据car90为例,该数据包括34个变量110条观察值。

```{r,warning=F}
library(rpart)
library(maptree)
data(car90)
#剔除轮胎尺寸、型号等3个因素型变量(factor variable): "Rim", "Tires", "Model2" 
cars = car90[, -match(c("Rim", "Tires", "Model2"), names(car90))]
#建立回归树模型
carfit = rpart(Price/1000 ~ ., data=cars)
carfit
printcp(carfit)
plot(carfit)
text(carfit)
#让图片好看一点
draw.tree(carfit)
```

###1.2.2 rweka包

J48()是基于C4.5算法的实现：

Weka里有非常全面的机器学习 算法，包括数据预处理、分类、回归、聚类、关联规则等。

```{r}
library(RWeka)

data(iris)
m1<-J48(Species~.,data=iris)
plot(m1)
#m1

```

##1.3练习

用solder数据做Possion回归树：

```{r,eval=F}
head(solder)
str(solder)
#建立poisson回归树 
sfit = rpart(skips ~ Opening + Solder + Mask + PadType + Panel,data = solder, method = 'poisson',control = rpart.control(cp = 0.05, maxcompete = 2))
sfit
printcp(sfit)
summary(sfit,cp=.1)
plot(sfit)
text(sfit)
draw.tree(sfit) 
table(iris$Species,predict(m1))

```

##1.4 延伸阅读

决策树是一个弱分类器，集成学习的方法可以将弱学习器组合在一起，根据投票法得到强学习器。感兴趣的童鞋可以进一步学习AdaBoost，Bagging,以及random forest。具体建议参考书籍：《机器学习》（周志华）。


#2. 线形模型

##2.1理论

###2.1.1基本形式

例如用d个属性描述示例$x=(x_1,x_2,...,x_d)$其中，$x_i$是x在第i个属性上的取值。

线性模型（linear model)就是试图用一个线性组合来描述：

$f(x)=w_1x_1+w_2x_2+...+w_dx_d+b$

我们在其他很多的课程中也接触过用线性模型去近似非线性模型（nonlinear model）。因为线性模型的较好的可解释性（comprehensibility）。

下面介绍几种经典的线性模型——回归任务，二分类以及多分类。

###2.1.2线形回归

“线性回归”（linear regression）试图学得一个线性模型以尽可能准确地预测实值输出标记线性回归试图学习得到

$f(x_i)=wx_i+b$,使得$f(x_i) \simeq y_i$

通常，我们用均方误差来衡量f(x)和y之间的差别。均方误差是回归中最常用的应用了性能指标，均方误差的几何意义就是欧氏距离。（最小二乘法的几何意义就是试图找到一条直线，让所有样本到直线的距离之和最小）。

线形模型虽然简单，但是却有丰富的变化，例如，对于输出标记是在指数尺度上变化时，可以用输出标记的对数作为线性模型逼近的目标，即：$ln(y)=w^Tx+b$ 这就是对数线性回归，但实质上是在求取输入空间到输出空间的非线性映射。

更一般地，考虑单调可微函数g()，令$y=g^{-1}(w^tx+b)$，这个模型就是GLM（广义线性）模型，其中$g(\cdot)$称为联系函数（link function）。显然对数线性模型就是广义线性模型在$g(\cdot)=ln(\cdot)$时的特例。

###2.1.3 对数几率回归（Logistic Regression）

广义线形模型在二分类问题中的应用。

很多时候我们需要作的是把一堆东西进行二分类，而非进行回归，而回归产生的函数是$z=w^Tx+b$，所以我们希望将z转化为0/1值，最为理想的就是单位阶跃函数。

<div align=center>![](D:/Rclub/机器学习/unitstep.png)</div>

然而单位阶跃函数并不是连续的，故而我们需要构造一个替代函数，常用的是$y=\frac{1}{1+e^{-z}}$，其中$z=w^Tx+b$。

<div align=center>![](D:/Rclub/机器学习/unitstep2.png)</div>

可以推出$ln \frac{y}{1-y}=w^Tx+b$，y可以看作正例的可能性，1-y看作反例的可能性。两者的比值$\frac{y}{1-y}$ 成为几率（odds），反映了x作为正例的相对可能性，对几率取对数，得到对数几率（logit）$ln \frac{y}{1-y}$

对应模型就为“对数几率回归”，注意，虽然它的名字是回归，但却是一种分类学习的方法。这种方法有很多优点，它不仅可预测出类别，也可以得到近似概率预测。这对于很多需要概率辅助决策的任务很有用。通常用极大似然估计的方法结合梯度下降法和牛顿法可以找出参数的最优解。

### 2.1.4 线形判别分析（LDA）

LDA是一种经典分类方法，也叫做Fisher判别分析，与PCA思想类似，但目的不同：PCA是找一个低维的子空间，样本投影在这个空间基本不丢失信息。而fisher是寻找这样的一个空间，样本投影在这个空间上，类内距离最小，类间距离最大。

LDA思想：把样例投影到一条直线，使得同样例尽可能靠近，异类尽可能远离。二维情况如图所示：

<div align=center>![](D:/Rclub/机器学习/LDA1.png)</div>

这里就会遇到一个问题——怎么来衡量这些投影点的“靠近”与“远离”程度呢？

要使得同类投影点的协方差尽可能小,即$w^{T}\Sigma_0 w+w^{T}\Sigma_1 w$

要使得异类投影点距离尽可能大，即$||w^T\mu_0-w^T\mu_1||_2^2$，其中$X_i,\mu_i,\Sigma_i$分别表示i类示例的集合，均值向量和协方差矩阵。因此，只要使得

$J=\frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^{T}\Sigma_0 w+w^{T}\Sigma_1 w}$

尽可能大就好，求解出对应的w就好。

LDA推广到多类的方法：若要分为m类，则投影到m-1维的空间，利用构造的“散度”作为评判准则再求解w。 

值得一提的是，LDA可从贝叶斯决策的角度来解释，可以证明，当两类数据同先验，满足高斯分布并且协方差相等时，LDA可以达到最优分类。

##2.2应用

###2.2.1 glm()函数

logistic Regression

经典的LR模型只能做二分类，但有一些从LR中衍生出来的方法也可以解决多分类问题，具体如下：

1.普通二分类 logistic 回归 用系统的 glm

2.因变量多分类 logistic 回归

a.有序分类因变量：用 MASS 包里的 polr
b.无序分类因变量：用 nnet 包里的 multinom

3.条件logistic回归 用 survival 包里的 clogit 

```{r,warning=F}
#这里举一个基础的例子
#由于LR是一种经典的二分类算法，先将三种类别的数据取出两类
data<-iris[51:150,]
names(data)<-c('sl','sw','pl','pw','species')
logit.fit<-glm(data$species~sw+pw,binomial(link='logit'),data=data)
logit.pred <- ifelse(predict(logit.fit) > 0,"virginica","versicolor")
table(data$species,logit.pred)

```


##2.3延伸阅读

多分类Logistic回归,Lasso Logistic回归
QDA


###2.2.2 MASS包中的lda()函数


```{r}
library(MASS)
data(iris)
iris.lda=lda(Species~.,data=iris)
iris.lda
table<-table(iris$Species,predict(iris.lda,iris)$class)
table
###判对率
sum(diag(prop.table(table)))

```

#3.支持向量机(SVM)


SVM与LR回归的联系？

##3.1理论

###3.1.1 基本原理

举个简单的例子。现在有一个二维平面，平面上有两种不同的数据，分别用圈和叉表示。由于这些数据是线性可分的，所以可以用一条直线将这两类数据分开，这条直线就相当于一个超平面，超平面一边的数据点所对应的y全是-1，另一边所对应的y全是1。

这个超平面可以用分类函数$f(x)=w^Tx+b$表示，当f(x)等于0时，x是位于超平面上的点，而f(x)大于0的点对应y=1的数据点，f(x)小于0的点对应y=-1的点，如下图所示：

<div align=center>![](D:/Rclub/机器学习/SVM1.png)</div>

换言之，在进行分类的时候，遇到一个新的数据点x，将x代入f(x)中，如果f(x)小于0则将x的类别赋为-1，如果f(x)大于0则将x的类别赋为1。

接下来的问题是，如何确定这个超平面呢？从直观上而言，这个超平面应该是最适合分开两类数据的直线。而判定“最适合”的标准就是这条直线离直线两边的数据的间隔最大。所以，得寻找有着最大间隔的超平面。

###3.1.2 函数间隔和几何间隔

在超平面$w^Tx+b=0$确定的情况下，$|w^Tx+b|$能够表示点x到距离超平面的远近，而通过观察$w^Tx+b$的符号与类标记y的符号是否一致可判断分类是否正确，所以，可以用$(y*(w^Tx+b))$的正负性来判定或表示分类的正确性。于此，我们便引出了函数间隔（functional margin）的概念。

定义函数间隔（用$\hat{\gamma}$表示）为：

$$\hat\gamma=y(w^Tx+b)=yf(x)$$

而超平面$(w , b)$关于T中所有样本点$(x_i,y_i)$的函数间隔最小值（其中，x是特征，y是结果标签，i表示第i个样本），便为超平面$(w,  b)$关于训练数据集T的函数间隔：

$$\hat\gamma=min \hat\gamma_i，i={1,...n}$$ 

但这样定义的函数间隔有问题，即如果成比例的改变w和b（如将它们改成2w和2b），则函数间隔的值f(x)却变成了原来的2倍（虽然此时超平面没有改变），所以只有函数间隔还远远不够。

事实上，我们可以对法向量w加些约束条件，从而引出真正定义点到超平面的距离--几何间隔（geometrical margin）的概念。

假定对于一个点x，令其垂直投影到超平面上的对应点为$x_0$，$w$是垂直于超平面的一个向量，$\gamma$为样本x到超平面的距离，如下图所示：


<div align=center>![](D:/Rclub/机器学习/SVM2.png)</div>

根据平面几何知识，有:

$$x=x_0+\gamma\frac{w}{||w||}$$

其中$||w||$为$w$的二阶范数（范数是一个类似于模的表示长度的概念），$\frac{w}{||w||}$是单位向量（一个向量除以它的模称之为单位向量）。

又由于$x_0$是超平面上的点，满足$f(x_0)=0$,代入超平面的方程$w^Tx+b=0$，可得$w^Tx_0+b=0$，即$w^Tx_0=-b$。

随即令$x=x_0+\gamma\frac{w}{||w||}$的两边同时乘以$w^T$，得出：

$$\gamma=\frac{w^Tx+b}{||w||}=\frac{f(x)}{||w||}$$

为了得到$\gamma$的绝对值，令$\gamma$乘上对应的类别y，即可得出几何间隔（用$\tilde\gamma$表示）的定义：

$$\tilde\gamma=y\gamma=\frac{\hat\gamma}{||w||}$$

从上述函数间隔和几何间隔的定义可以看出：几何间隔就是函数间隔除以$||w||$，而且函数间隔实际上就是$|f(x)|$，只是人为定义的一个间隔度量，而几何间隔才是直观上的点到超平面的距离。

###3.1.3 最大间隔分类器

对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的确信度（confidence）也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个“间隔”值。这个间隔就是下图中的Gap的一半。


这里要找的最大间隔分类超平面中的“间隔”指的是几何间隔。

<div align=center>![](D:/Rclub/机器学习/SVM3.png)</div>

于是最大间隔分类器（maximum margin classifier）的目标函数可以定义为：

$$max\tilde\gamma$$

约束条件为：

$$y_i(w^Tx_i+b)=\hat\gamma_i\geq \hat\gamma，i={1,...,n}$$

令函数间隔$\tilde\gamma=1$，对优化结果没有影响，则优化问题转换为：
$$max\frac{1}{||w||},s.t., y_i(w^Tx_i+b)\geq 1,i={1,...,n}$$

相当于在上述约束条件下最大化$\frac{1}{||w||}$，而$\frac{1}{||w||}$便是几何间隔$\tilde\gamma$。

<div align=center>![](D:/Rclub/机器学习/SVM4.png)</div>

###3.1.4 线形不可分(Kernel 核函数)

到目前为止，我们的SVM还比较弱，只能处理线性的情况，下面我们将引入核函数，进而推广到非线性分类问题。

大部分时候数据并不是线性可分的，这个时候满足这样条件的超平面就根本不存在。在上文中，我们已经了解到了SVM处理线性可分的情况，那对于非线性的数据SVM咋处理呢？对于非线性的情况，SVM 的处理方法是选择一个核函数 κ(⋅,⋅) ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。

具体来说，在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。如所示，一堆数据在二维空间无法划分，从而映射到三维空间里划分：

<div align=center>![](D:/Rclub/机器学习/SVM5.png)</div>

几个核函数:

（1）多项式核函数
（2）高斯核函数
（3）线形核函数


<div align=center>![](D:/Rclub/机器学习/SVM6.png)</div>

引用一个例子举例说明下核函数解决非线性问题的直观效果。

假设现在你是一个农场主，养了一批羊群，但为预防狼群袭击羊群，如何搭建一个篱笆来把羊群和狼分开呢？


<div align=center>![](D:/Rclub/机器学习/SVM7.png)</div>

这个例子从侧面简单说明了SVM使用非线性分类器的优势，而逻辑模式以及决策树模式都是使用了直线方法。

##3.2应用

用e1071程序包所提供svm()函数来实现：

```{r,warning=F}
library(lattice)
library(e1071)
data(iris)
xyplot(Petal.Length~Petal.Width,data=iris,groups=Species,auto.key=list(corner=c(1,0)))
#标记为setosa的鸢尾花可以很容易地被划分出来。但仅使用Petal.Length和Petal.Width这两个特征时，versicolor和virginica之间尚不是线性可分的。接下来我们用svm来进行分类：
subdata<-iris[iris$Species!='virginica',]
subdata$Species<-factor(subdata$Species)
iris.svm<-svm(Species~Petal.Length+Petal.Width,data=subdata)
summary(iris.svm)
plot(iris.svm,subdata,Petal.Length~Petal.Width)
pred <- predict(iris.svm,subdata[,1:4])
table(pred,subdata$Species)

```

参考文献：http://blog.csdn.net/v_july_v/article/details/7624837

